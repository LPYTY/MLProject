{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAStwfrCv6TL",
        "outputId": "6f30112c-aecc-4468-a2e7-7105a74a86bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "#!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lLENvwwCvOuE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "package_used = 'torch'\n",
        "\n",
        "def uran_numpy(n):\n",
        "    return np.random.rand(n)\n",
        "\n",
        "def uran_torch(n):\n",
        "    return torch.rand(n)\n",
        "\n",
        "def gran_numpy(n, m):\n",
        "    return np.random.randn(n, m)\n",
        "\n",
        "def gran_torch(n, m):\n",
        "    return torch.randn(n, m)\n",
        "'''\n",
        "def clp_numpy(n, B, x):\n",
        "    u_hat = np.zeros(n)\n",
        "    C = np.inf\n",
        "    i = n + 1\n",
        "    d = np.full(n, n)\n",
        "    lamb = np.zeros(n + 1)\n",
        "    F = np.zeros((n, n))\n",
        "    F[-1, :] = x\n",
        "    u = np.zeros(n)\n",
        "    p = np.zeros(n)\n",
        "    delta = np.zeros(n)\n",
        "    while True:\n",
        "        while True:\n",
        "            if i != 1:\n",
        "                i -= 1\n",
        "                for j in range(d[i - 1], i + 1, -1):\n",
        "                    F[j - 2, i - 1] = F[j - 1, i - 1] - u[j - 1] * B[j - 1, i - 1]\n",
        "                p[i - 1] = F[i - 1, i - 1] / B[i - 1, i - 1]\n",
        "                u[i - 1] = np.round(p[i - 1])\n",
        "                y = (p[i - 1] - u[i - 1]) * B[i - 1, i - 1]\n",
        "                delta[i - 1] = np.sign(y)\n",
        "                lamb[i - 1] = lamb[i] + y * y\n",
        "            else:\n",
        "                u_hat = u\n",
        "                C = lamb[0]\n",
        "            if lamb[i - 1] >= C:\n",
        "                break\n",
        "        m = i\n",
        "        while True:\n",
        "            if i == n:\n",
        "                return u_hat\n",
        "            i += 1\n",
        "            u[i - 1] = u[i - 1] + delta[i - 1]\n",
        "            delta[i - 1] = -delta[i - 1] - np.sign(delta[i - 1])\n",
        "            y = (p[i - 1] - u[i - 1]) * B[i - 1, i - 1]\n",
        "            lamb[i - 1] = lamb[i] + y * y\n",
        "            if lamb[i - 1] < C:\n",
        "                break\n",
        "        #for j in range(m, i - 1):\n",
        "        #    d[j - 1] = i\n",
        "        d[m - 1: i - 1] = i\n",
        "        for j in range(m - 1, 0, -1):\n",
        "            if d[j - 1] < i:\n",
        "                d[j - 1] = i\n",
        "            else:\n",
        "                break\n",
        "\n",
        "def clp_torch(n, B, x):\n",
        "    u_hat = torch.zeros(n, dtype=B.dtype, device=B.device)\n",
        "    C = torch.tensor(np.inf, dtype=B.dtype, device=B.device)\n",
        "    i = n + 1\n",
        "    d = torch.full((n,), n, dtype=torch.long, device=B.device)\n",
        "    lamb = torch.zeros(n + 1, dtype=B.dtype, device=B.device)\n",
        "    F = torch.zeros((n, n), dtype=B.dtype, device=B.device)\n",
        "    F[-1, :] = x\n",
        "    u = torch.zeros(n, dtype=B.dtype, device=B.device)\n",
        "    p = torch.zeros(n, dtype=B.dtype, device=B.device)\n",
        "    delta = torch.zeros(n, dtype=B.dtype, device=B.device)\n",
        "    while True:\n",
        "        while True:\n",
        "            if i != 1:\n",
        "                i -= 1\n",
        "                for j in range(d[i - 1], i + 1, -1):\n",
        "                    F[j - 2, i - 1] = F[j - 1, i - 1] - u[j - 1] * B[j - 1, i - 1]\n",
        "                p[i - 1] = F[i - 1, i - 1] / B[i - 1, i - 1]\n",
        "                u[i - 1] = torch.round(p[i - 1])\n",
        "                y = (p[i - 1] - u[i - 1]) * B[i - 1, i - 1]\n",
        "                delta[i - 1] = torch.sign(y)\n",
        "                lamb[i - 1] = lamb[i] + y * y\n",
        "            else:\n",
        "                u_hat = u\n",
        "                C = lamb[0]\n",
        "            if lamb[i - 1] >= C:\n",
        "                break\n",
        "        m = i\n",
        "        while True:\n",
        "            if i == n:\n",
        "                return u_hat\n",
        "            i += 1\n",
        "            u[i - 1] = u[i - 1] + delta[i - 1]\n",
        "            delta[i - 1] = -delta[i - 1] - torch.sign(delta[i - 1])\n",
        "            y = (p[i - 1] - u[i - 1]) * B[i - 1, i - 1]\n",
        "            lamb[i - 1] = lamb[i] + y * y\n",
        "            if lamb[i - 1] < C:\n",
        "                break\n",
        "        d[m - 1: i - 1] = i\n",
        "        for j in range(m - 1, 0, -1):\n",
        "            if d[j - 1] < i:\n",
        "                d[j - 1] = i\n",
        "            else:\n",
        "                break\n",
        "'''\n",
        "def initial_gso_numpy(basis):\n",
        "    \"\"\"\n",
        "    Compute the initial Gram–Schmidt decomposition of a row-based 'basis'.\n",
        "\n",
        "    Returns:\n",
        "      b_star : (n, dim) array of the orthogonal vectors b_i^*\n",
        "      mu     : (n, n)   array, mu[i,j] = coefficient of b_j^* in b_i, for j < i\n",
        "      B      : (n,)     array, B[j] = ||b_j^*||^2\n",
        "    \"\"\"\n",
        "    n, dim = basis.shape\n",
        "    b_star = np.zeros((n, dim), dtype=basis.dtype)\n",
        "    mu     = np.zeros((n, n),    dtype=basis.dtype)\n",
        "    B      = np.zeros(n,         dtype=basis.dtype)  # squared norms of b_j^*\n",
        "\n",
        "    for i in range(n):\n",
        "        # Start from b_i\n",
        "        b_star[i] = basis[i].copy()\n",
        "        for j in range(i):\n",
        "            # Project b_i onto b_j^*\n",
        "            mu[i, j] = np.dot(basis[i], b_star[j]) / B[j] if B[j] != 0 else 0\n",
        "            b_star[i] -= mu[i, j] * b_star[j]\n",
        "        B[i] = np.dot(b_star[i], b_star[i])\n",
        "\n",
        "    return b_star, mu, B\n",
        "\n",
        "def size_reduce_numpy(basis, b_star, mu, B, k, j):\n",
        "    \"\"\"\n",
        "    Size-reduce row k with respect to row j using partial updates.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    basis  : (n, dim)  row-based array of the basis\n",
        "    b_star : (n, dim)  row-based array of the orthogonal vectors\n",
        "    mu     : (n, n)    GSO coefficients\n",
        "    B      : (n,)      squared norms of b_star\n",
        "    k      : int       index of row being reduced\n",
        "    j      : int       index of row used for reduction\n",
        "    \"\"\"\n",
        "    q = np.round(mu[k, j])\n",
        "    if np.isclose(q, 0.0):\n",
        "        return  # no reduction needed\n",
        "\n",
        "    # 1. Update b_k by subtracting q * b_j\n",
        "    basis[k] -= q * basis[j]\n",
        "\n",
        "    # 2. Update the GSO coefficients mu[k, t] for t in [0..j-1]\n",
        "    for t in range(j):\n",
        "        mu[k, t] -= q * mu[j, t]\n",
        "\n",
        "    # 3. Update mu[k, j] by subtracting q\n",
        "    mu[k, j] -= q\n",
        "\n",
        "    # 4. Update b_k^*\n",
        "    b_star[k] -= q * b_star[j]\n",
        "    # 5. Update the squared norm B[k]\n",
        "    B[k] = np.dot(b_star[k], b_star[k])\n",
        "\n",
        "def update_gso_after_swap_numpy(basis, b_star, mu, B, k):\n",
        "    \"\"\"\n",
        "    After swapping rows k and k-1 in 'basis', we do a *partial* fix-up:\n",
        "      - Swap b_star[k] and b_star[k-1]\n",
        "      - Swap mu-coefficients with indices (k, :) and (k-1, :)\n",
        "      - Recompute mu[k,k-1], b_star[k], B[k], and possibly some mu[*][k].\n",
        "    \"\"\"\n",
        "    # Swap the orthogonal vectors b_star[k] and b_star[k-1]\n",
        "    temp = b_star[k].copy()\n",
        "    b_star[k] = b_star[k-1]\n",
        "    b_star[k-1] = temp\n",
        "\n",
        "    # Swap the squared norms as well\n",
        "    B[k], B[k-1] = B[k-1], B[k]\n",
        "\n",
        "    # Swap all mu[k, t] <--> mu[k-1, t], for t < k-1\n",
        "    for t in range(k-1):\n",
        "        mu[k, t], mu[k-1, t] = mu[k-1, t], mu[k, t]\n",
        "\n",
        "    # Now we must *recompute* mu[k, k-1], b_star[k], and B[k].\n",
        "    # Because b_k changed to what b_{k-1} was, and vice versa.\n",
        "\n",
        "    # Recompute mu[k,k-1] = (b_k dot b_{k-1}^*) / ||b_{k-1}^*||^2\n",
        "    mu[k, k-1] = np.dot(basis[k], b_star[k-1]) / B[k-1] if B[k-1] != 0 else 0.0\n",
        "\n",
        "    # Recompute b_star[k] = b_k - sum_{j=0}^{k-1} mu[k,j]*b_star[j]\n",
        "    # but the partial trick is to only fix b_star[k] now that we've updated mu[k,k-1].\n",
        "\n",
        "    # 1) Start fresh:\n",
        "    b_star[k] = basis[k].copy()\n",
        "    for j in range(k):\n",
        "        b_star[k] -= mu[k, j] * b_star[j]\n",
        "\n",
        "    # 2) Update B[k]\n",
        "    B[k] = np.dot(b_star[k], b_star[k])\n",
        "\n",
        "def lll_partial_update_numpy(basis, delta=0.75):\n",
        "    \"\"\"\n",
        "    LLL reduction with partial GSO updates.\n",
        "    Each row of 'basis' is a vector in R^dim.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    basis : (n, dim) array\n",
        "      Row-based representation of the lattice basis.\n",
        "    delta : float\n",
        "      Lovasz condition parameter in (0.25, 1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    basis : (n, dim) array\n",
        "      LLL-reduced basis (modified in place and also returned).\n",
        "    \"\"\"\n",
        "    # Convert to floating type for dot products etc.\n",
        "    # basis = basis.astype(float)\n",
        "    n, dim = basis.shape\n",
        "\n",
        "    # -- 1) Compute initial GSO:\n",
        "    b_star, mu, B = initial_gso_numpy(basis)\n",
        "\n",
        "    # -- 2) LLL main loop:\n",
        "    k = 1\n",
        "    while k < n:\n",
        "        # (a) Size Reduction: reduce b_k w.r.t. b_j for j = k-1..0\n",
        "        for j in range(k-1, -1, -1):\n",
        "            size_reduce_numpy(basis, b_star, mu, B, k, j)\n",
        "\n",
        "        # (b) Check Lovász condition if k >= 1\n",
        "        #     i.e.,  ||b_k^*||^2 >= (delta - mu[k,k-1]^2)*||b_{k-1}^*||^2\n",
        "        if k > 0:\n",
        "            lhs = B[k]\n",
        "            rhs = (delta - mu[k, k-1]**2) * B[k-1]\n",
        "            if lhs < rhs:\n",
        "                # Swap rows k and k-1 in 'basis'\n",
        "                basis[[k, k-1]] = basis[[k-1, k]]\n",
        "\n",
        "                # Partial GSO fix-up after the swap\n",
        "                update_gso_after_swap_numpy(basis, b_star, mu, B, k)\n",
        "\n",
        "                # Step back\n",
        "                k = max(k-1, 1)\n",
        "                continue\n",
        "\n",
        "        # Move forward\n",
        "        k += 1\n",
        "\n",
        "    return basis\n",
        "\n",
        "def initial_gso_torch(basis: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Compute the initial Gram–Schmidt decomposition of a row-based 'basis'.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    basis : (n, dim) torch.Tensor\n",
        "      Each row of 'basis' is a vector in R^dim.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    b_star : (n, dim) torch.Tensor\n",
        "      Orthogonalized vectors b_i^* for each row b_i.\n",
        "    mu     : (n, n) torch.Tensor\n",
        "      mu[i, j] = (b_i dot b_j^*) / ||b_j^*||^2 for j < i, else 0\n",
        "    B      : (n,) torch.Tensor\n",
        "      B[j] = ||b_j^*||^2 (the squared norm of b_j^*)\n",
        "    \"\"\"\n",
        "    n, dim = basis.shape\n",
        "\n",
        "    b_star = torch.zeros_like(basis)\n",
        "    mu = torch.zeros(n, n, dtype=basis.dtype, device=basis.device)\n",
        "    B  = torch.zeros(n,    dtype=basis.dtype, device=basis.device)\n",
        "\n",
        "    for i in range(n):\n",
        "        # Start with b_i:\n",
        "        b_star[i] = basis[i].clone()\n",
        "        for j in range(i):\n",
        "            # Project b_i onto b_j^*\n",
        "            # mu[i,j] = (b_i dot b_j^*) / ||b_j^*||^2\n",
        "            denom = B[j]\n",
        "            if denom.abs() > 1e-15:  # avoid division by zero\n",
        "                mu[i, j] = torch.dot(basis[i], b_star[j]) / denom\n",
        "            else:\n",
        "                mu[i, j] = torch.tensor(0.0, device=basis.device)\n",
        "\n",
        "            # Subtract this projection from b_star[i]\n",
        "            b_star[i] -= mu[i, j] * b_star[j]\n",
        "\n",
        "        # B[i] = ||b_star[i]||^2\n",
        "        B[i] = torch.dot(b_star[i], b_star[i])\n",
        "\n",
        "    return b_star, mu, B\n",
        "\n",
        "def size_reduce_torch(basis, b_star, mu, B, k, j):\n",
        "    \"\"\"\n",
        "    Size-reduce row k with respect to row j using partial updates in PyTorch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    basis  : (n, dim) torch.Tensor\n",
        "      Row-based array of the basis vectors.\n",
        "    b_star : (n, dim) torch.Tensor\n",
        "      Row-based array of the orthogonal vectors.\n",
        "    mu     : (n, n)   torch.Tensor\n",
        "      GSO coefficients.\n",
        "    B      : (n,)     torch.Tensor\n",
        "      Squared norms of b_star.\n",
        "    k      : int\n",
        "      Index of row being reduced.\n",
        "    j      : int\n",
        "      Index of row used for reduction.\n",
        "    \"\"\"\n",
        "    q = torch.round(mu[k, j])\n",
        "    # We'll treat \"close to zero\" as zero\n",
        "    if torch.abs(q) < 1e-15:\n",
        "        return  # no reduction needed\n",
        "\n",
        "    # 1) Update b_k: b_k = b_k - q * b_j\n",
        "    basis[k] -= q * basis[j]\n",
        "\n",
        "    # 2) Update the GSO coefficients mu[k, t] for t in [0..j-1]\n",
        "    #    mu[k, t] = mu[k, t] - q * mu[j, t]\n",
        "    mu[k, :j] -= q * mu[j, :j]\n",
        "\n",
        "    # 3) Update mu[k, j] by subtracting q\n",
        "    mu[k, j] -= q\n",
        "\n",
        "    # 4) Update b_k^*\n",
        "    b_star[k] -= q * b_star[j]\n",
        "\n",
        "    # 5) Update the squared norm B[k]\n",
        "    B[k] = torch.dot(b_star[k], b_star[k])\n",
        "\n",
        "def update_gso_after_swap_torch(basis, b_star, mu, B, k):\n",
        "    \"\"\"\n",
        "    After swapping rows k and k-1 in 'basis', do a partial fix-up of the GSO data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    basis  : (n, dim) torch.Tensor\n",
        "      Row-based basis matrix.\n",
        "    b_star : (n, dim) torch.Tensor\n",
        "      Orthogonal vectors.\n",
        "    mu     : (n, n)   torch.Tensor\n",
        "      GSO coefficients.\n",
        "    B      : (n,)     torch.Tensor\n",
        "      Squared norms of b_star.\n",
        "    k      : int\n",
        "      The row index that was swapped with (k-1).\n",
        "    \"\"\"\n",
        "    # 1) Swap b_star[k] and b_star[k-1]\n",
        "    temp = b_star[k].clone()\n",
        "    b_star[k] = b_star[k-1]\n",
        "    b_star[k-1] = temp\n",
        "\n",
        "    # 2) Swap B[k] and B[k-1]\n",
        "    tempB = B[k].clone()\n",
        "    B[k]   = B[k-1]\n",
        "    B[k-1] = tempB\n",
        "\n",
        "    # 3) Swap mu[k,:] and mu[k-1,:], but only for columns < k-1\n",
        "    #    i.e., mu[k, t] <--> mu[k-1, t]\n",
        "    #    for t in [0..k-2]\n",
        "    tmp_slice = mu[k, :k-1].clone()\n",
        "    mu[k, :k-1] = mu[k-1, :k-1]\n",
        "    mu[k-1, :k-1] = tmp_slice\n",
        "\n",
        "    # 4) Recompute mu[k,k-1], b_star[k], and B[k].\n",
        "    denom = B[k-1]\n",
        "    if denom.abs() > 1e-15:\n",
        "        mu[k, k-1] = torch.dot(basis[k], b_star[k-1]) / denom\n",
        "    else:\n",
        "        mu[k, k-1] = torch.tensor(0.0, device=basis.device)\n",
        "\n",
        "    # 5) Recompute b_star[k] = b_k - sum_{j=0}^{k} mu[k,j] * b_star[j]\n",
        "    b_star[k] = basis[k].clone()\n",
        "    for j in range(k):\n",
        "        b_star[k] -= mu[k, j] * b_star[j]\n",
        "\n",
        "    # 6) Update B[k] = ||b_star[k]||^2\n",
        "    B[k] = torch.dot(b_star[k], b_star[k])\n",
        "\n",
        "def lll_partial_update_torch(basis, delta=0.75):\n",
        "    \"\"\"\n",
        "    LLL reduction with partial GSO updates (PyTorch version).\n",
        "    Each row of 'basis' is a vector in R^dim.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    basis : (n, dim) torch.Tensor\n",
        "      Row-based representation of the lattice basis.\n",
        "    delta : float\n",
        "      Lovasz condition parameter in (0.25, 1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    basis : (n, dim) torch.Tensor\n",
        "      LLL-reduced basis (modified in-place and also returned).\n",
        "    \"\"\"\n",
        "    # Make sure it's float (or double)\n",
        "    # basis = basis.to(dtype=torch.double)  # double precision for safety\n",
        "\n",
        "    # -- 1) Initial GSO\n",
        "    b_star, mu, B = initial_gso_torch(basis)\n",
        "\n",
        "    n = basis.shape[0]\n",
        "    k = 1\n",
        "    while k < n:\n",
        "        # -- (a) Size Reduction: reduce b_k w.r.t. b_j for j = k-1..0\n",
        "        for j in range(k-1, -1, -1):\n",
        "            size_reduce_torch(basis, b_star, mu, B, k, j)\n",
        "\n",
        "        # -- (b) Check Lovasz condition if k >= 1\n",
        "        #     ||b_k^*||^2 >= (delta - mu[k,k-1]^2) * ||b_{k-1}^*||^2\n",
        "        if k > 0:\n",
        "            lhs = B[k]\n",
        "            rhs = (delta - mu[k, k-1]**2) * B[k-1]\n",
        "\n",
        "            if lhs < rhs:\n",
        "                # Swap rows k and k-1 in 'basis'\n",
        "                tmp_row = basis[k].clone()\n",
        "                basis[k] = basis[k-1]\n",
        "                basis[k-1] = tmp_row\n",
        "\n",
        "                # Partial GSO fix-up\n",
        "                update_gso_after_swap_torch(basis, b_star, mu, B, k)\n",
        "\n",
        "                # Step back\n",
        "                k = max(k-1, 1)\n",
        "                continue\n",
        "\n",
        "        k += 1\n",
        "\n",
        "    return basis\n",
        "\n",
        "def orth_numpy(B):\n",
        "    A = B @ B.T\n",
        "    return np.linalg.cholesky(A)\n",
        "\n",
        "def orth_torch(B):\n",
        "    A = B @ B.T\n",
        "    return torch.linalg.cholesky(A).type(B.dtype)\n",
        "\n",
        "def uran(n):\n",
        "    if package_used == 'numpy':\n",
        "        return uran_numpy(n)\n",
        "    else:\n",
        "        return uran_torch(n)\n",
        "\n",
        "def gran(n, m):\n",
        "    if package_used == 'numpy':\n",
        "        return gran_numpy(n, m)\n",
        "    else:\n",
        "        return gran_torch(n, m)\n",
        "'''\n",
        "def clp(n, B, x):\n",
        "    if package_used == 'numpy':\n",
        "        return clp_numpy(n, B, x)\n",
        "    else:\n",
        "        return clp_torch(n, B, x)\n",
        "'''\n",
        "def red(B):\n",
        "    if package_used == 'numpy':\n",
        "        return lll_partial_update_numpy(B)\n",
        "    else:\n",
        "        return lll_partial_update_torch(B)\n",
        "\n",
        "def orth(B):\n",
        "    if package_used == 'numpy':\n",
        "        return orth_numpy(B)\n",
        "    else:\n",
        "        return orth_torch(B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lhUvU3kvP1s"
      },
      "source": [
        "# 新段落"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wPvHO-UjgNPu"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def CLP(B,r):\n",
        "    n=r.shape[0]\n",
        "    c=1e12\n",
        "    i=n+1\n",
        "    d=n*np.ones((n+1),dtype=np.int32)\n",
        "    lmda=np.zeros((n+2),dtype=np.float32)\n",
        "    F=np.zeros((n+1,n+1),dtype=np.float32)\n",
        "    F[n,1:]=r.copy()\n",
        "    u=np.zeros((n+1),dtype=np.int32)\n",
        "    u_ans=np.zeros((n+1),dtype=np.int32)\n",
        "    det=np.zeros((n+1),dtype=np.float32)\n",
        "    p=np.zeros((n+1),dtype=np.float32)\n",
        "    start_time = time.time()\n",
        "    while True:\n",
        "        cur_time = time.time()\n",
        "        if cur_time - start_time > 10:\n",
        "            return u_ans[1:]\n",
        "        while(lmda[i]<c):\n",
        "            if i!=1:\n",
        "                i=i-1\n",
        "                for j in range(d[i],i,-1):\n",
        "                    F[j-1,i]=F[j,i]-u[j]*B[j-1,i-1]\n",
        "                p[i]=F[i,i]/B[i-1,i-1]\n",
        "                u[i]=round(p[i])\n",
        "                y=(p[i]-u[i])*B[i-1,i-1]\n",
        "                det[i]=np.sign(y)\n",
        "                lmda[i]=lmda[i+1]+y**2\n",
        "            else:\n",
        "                u_ans=u.copy()\n",
        "                c=lmda[1].copy()\n",
        "        m=i\n",
        "        while(lmda[i]>=c):\n",
        "            if i==n:\n",
        "                return u_ans[1:]\n",
        "            else:\n",
        "                i=i+1\n",
        "                u[i]+=det[i]\n",
        "                det[i]=-det[i]-np.sign(det[i])\n",
        "                y=(p[i]-u[i])*B[i-1,i-1]\n",
        "                lmda[i]=lmda[i+1]+y**2\n",
        "        for j in range(m,i):\n",
        "            d[j]=i\n",
        "        for j in range(1,m):\n",
        "            if d[j]<i:\n",
        "                d[j]=i\n",
        "            else:\n",
        "                break\n",
        "\n",
        "import torch\n",
        "\n",
        "def CLP_torch(B, r):\n",
        "    n = r.shape[0]\n",
        "    c = torch.tensor(np.inf, device=B.device)\n",
        "    i = n + 1\n",
        "    d = n * torch.ones(n + 1, dtype=torch.int32, device=B.device)\n",
        "    lmda = torch.zeros(n + 2, dtype=torch.float32, device=B.device)\n",
        "    F = torch.zeros(n + 1, n + 1, dtype=torch.float32, device=B.device)\n",
        "    F[n, 1:] = r.clone()\n",
        "    u = torch.zeros(n + 1, dtype=torch.int32, device=B.device)\n",
        "    u_ans = torch.zeros(n + 1, dtype=torch.int32, device=B.device)\n",
        "    det = torch.zeros(n + 1, dtype=torch.int32, device=B.device)\n",
        "    p = torch.zeros(n + 1, dtype=torch.float32, device=B.device)\n",
        "    start_time = time.time()\n",
        "    while True:\n",
        "        cur_time = time.time()\n",
        "        if cur_time - start_time > 10:\n",
        "            return u_ans[1:]\n",
        "        while True:\n",
        "            if i != 1:\n",
        "                i = i - 1\n",
        "                for j in range(d[i], i, -1):\n",
        "                    F[j - 1, i] = F[j, i] - u[j] * B[j - 1, i - 1]\n",
        "                p[i] = F[i, i] / B[i - 1, i - 1]\n",
        "                u[i] = torch.round(p[i]).int()\n",
        "                y = (p[i] - u[i]) * B[i - 1, i - 1]\n",
        "                det[i] = torch.sign(y)\n",
        "                lmda[i] = lmda[i + 1] + y**2\n",
        "            else:\n",
        "                u_ans = u.clone()\n",
        "                c = lmda[1].clone()\n",
        "            if lmda[i] >= c:\n",
        "                break\n",
        "        m = i\n",
        "        while True:\n",
        "            if i == n:\n",
        "                return u_ans[1:]\n",
        "            else:\n",
        "                i = i + 1\n",
        "                u[i] += det[i]\n",
        "                det[i] = -det[i] - torch.sign(det[i])\n",
        "                y = (p[i] - u[i]) * B[i - 1, i - 1]\n",
        "                lmda[i] = lmda[i + 1] + y**2\n",
        "            if lmda[i] < c:\n",
        "                break\n",
        "        for j in range(m, i):\n",
        "            d[j] = i\n",
        "        for j in range(1, m):\n",
        "            if d[j] < i:\n",
        "                d[j] = i\n",
        "            else:\n",
        "                break\n",
        "\n",
        "def clp(n, B, x):\n",
        "    if package_used == 'numpy':\n",
        "        return CLP(B, x)\n",
        "    else:\n",
        "        return CLP_torch(B, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_j2YwXx6XFg",
        "outputId": "9e4fe861-7e41-4cce-e4e4-58b42ebbcde8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created Models\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "#from google.colab import drive\n",
        "from pathlib import Path\n",
        "#drive.mount('/content/gdrive')\n",
        "#model_path = '/content/gdrive/MyDrive/ML/Models/'\n",
        "model_path = \"./Models/\"\n",
        "model_path = Path(model_path)\n",
        "if not model_path.exists():\n",
        "    model_path.mkdir(parents=True)\n",
        "    print(f'Created {model_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F9P3IqEPvgMF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "#from utils import clp, red, orth\n",
        "#import utils\n",
        "\n",
        "#utils.package_used = 'torch'\n",
        "package_used='torch'\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.n = 10\n",
        "        self.d = 2\n",
        "        self.k = 2\n",
        "        self.num_epochs = 1000000\n",
        "        self.red_epoch_interval = 100\n",
        "        self.batch_size = 1\n",
        "        self.val_size = 16\n",
        "        self.lr = 0.001\n",
        "        self.seed = None\n",
        "        self.device = 'cpu'#torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.dtype = torch.float32\n",
        "        self.reduce_B = True\n",
        "        self.save_interval = 10000\n",
        "\n",
        "def loss_fn(n, V, e):\n",
        "    return 1 / n * (V ** (-2 / n)) * torch.norm(e, p=2) ** 2\n",
        "\n",
        "class ILC(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(ILC, self).__init__()\n",
        "        self.config = config\n",
        "        self.n = config.n\n",
        "        self.B = nn.Parameter(torch.randn(self.n, self.n, dtype=config.dtype, device=config.device))\n",
        "        self.B.data = self.reduce_B(self.B.data)\n",
        "        self.B.data = torch.diag(torch.ones(config.n, dtype=config.dtype, device=config.device))\n",
        "        self.mask = self.generate_mask(self.n)\n",
        "\n",
        "    def calculate_V(self, B):\n",
        "        return torch.prod(torch.diag(B))\n",
        "\n",
        "    def reduce_B(self, B):\n",
        "        with torch.no_grad():\n",
        "            B = orth(red(B))\n",
        "            V = self.calculate_V(B)\n",
        "            B = V ** (-1 / self.n) * B\n",
        "        return B\n",
        "\n",
        "    def generate_mask(self, n):\n",
        "        # Keep B as a lower triangular matrix\n",
        "        mask = torch.tril(torch.ones(n, n, dtype=torch.bool, device=self.config.device))\n",
        "        return mask\n",
        "\n",
        "    def mask_B(self, B):\n",
        "        with torch.no_grad():\n",
        "            B = B * self.mask\n",
        "        return B\n",
        "\n",
        "    def forward(self, y):\n",
        "        e = y @ self.B\n",
        "        V = self.calculate_V(self.B)\n",
        "        loss = loss_fn(self.n, V, e)\n",
        "        return loss\n",
        "\n",
        "def calculate_y(n, z, B):\n",
        "    with torch.no_grad():\n",
        "        z = z.to(B.device)\n",
        "        B = B.to(B.device)\n",
        "        return z - clp(n, B, z @ B)\n",
        "\n",
        "def iterative_lattice_construction(n):\n",
        "    config = Config()\n",
        "    config.n = n\n",
        "    model = ILC(config)\n",
        "    model.to(config.device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.lr)\n",
        "    scheduler = StepLR(optimizer, step_size=10000, gamma=0.9)\n",
        "    if config.seed is not None:\n",
        "        torch.manual_seed(config.seed)\n",
        "    else:\n",
        "        torch.seed()\n",
        "    last_total_loss = 0\n",
        "    for epoch in tqdm(range(config.num_epochs)):\n",
        "        loss = torch.tensor(0, dtype=config.dtype, device=config.device)\n",
        "        z = torch.rand(config.batch_size, config.n, dtype=config.dtype, device=config.device)\n",
        "        for i in range(config.batch_size):\n",
        "            y = calculate_y(config.n, z[i], model.B.data)\n",
        "            loss += model(y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.B.data = model.mask_B(model.B.data)\n",
        "        #last_total_loss += loss.item() / config.batch_size\n",
        "        #print('Epoch: {}, Loss: {}'.format(epoch, loss.item() / config.batch_size))\n",
        "        if epoch % config.red_epoch_interval == config.red_epoch_interval - 1 and config.reduce_B:\n",
        "            model.B.data = model.reduce_B(model.B.data)\n",
        "            #with torch.no_grad():\n",
        "            #  loss = torch.tensor(0, dtype=config.dtype, device=config.device)\n",
        "            #  z = torch.rand(config.val_size, config.n, dtype=config.dtype, device=config.device)\n",
        "            #  for i in range(config.val_size):\n",
        "            #    y = calculate_y(config.n, z[i], model.B.data)\n",
        "            #    loss += model(y)\n",
        "            #print()\n",
        "            #print(f'Epoch {epoch}, Loss: {loss.item() / config.batch_size}')\n",
        "            last_total_loss = 0\n",
        "\n",
        "            #print('Reduced B')\n",
        "        if epoch % config.save_interval == config.save_interval - 1:\n",
        "            print()\n",
        "            #print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "            last_total_loss = 0\n",
        "            torch.save(model.B, model_path / f'B_{model.config.n}_{epoch}.pt')\n",
        "            print(f'Saved B_{model.config.n}_{epoch}.pt')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpXARdvbvuox",
        "outputId": "452e5c97-e2d3-4c13-b7e8-73859ef66651"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 9703/1000000 [02:40<4:32:42, 60.52it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model: run this cell\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43miterative_lattice_construction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mB)\n\u001b[0;32m      5\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model, model_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[7], line 92\u001b[0m, in \u001b[0;36miterative_lattice_construction\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m     90\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(config\u001b[38;5;241m.\u001b[39mbatch_size, config\u001b[38;5;241m.\u001b[39mn, dtype\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m---> 92\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model(y)\n\u001b[0;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "Cell \u001b[1;32mIn[7], line 74\u001b[0m, in \u001b[0;36mcalculate_y\u001b[1;34m(n, z, B)\u001b[0m\n\u001b[0;32m     72\u001b[0m z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mto(B\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     73\u001b[0m B \u001b[38;5;241m=\u001b[39m B\u001b[38;5;241m.\u001b[39mto(B\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z \u001b[38;5;241m-\u001b[39m \u001b[43mclp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[2], line 98\u001b[0m, in \u001b[0;36mclp\u001b[1;34m(n, B, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CLP(B, x)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCLP_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[2], line 90\u001b[0m, in \u001b[0;36mCLP_torch\u001b[1;34m(B, r)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, m):\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d[j] \u001b[38;5;241m<\u001b[39m i:\n\u001b[1;32m---> 90\u001b[0m         d[j] \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train the model: run this cell\n",
        "if __name__ == '__main__':\n",
        "    model = iterative_lattice_construction(10)\n",
        "    print(model.B)\n",
        "    torch.save(model, model_path / f'B_{model.config.n}_{time.time()}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvr2rkuJv_YB",
        "outputId": "cbdde94b-3c68-44fa-f4e5-a4598dd6d704"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-52-bcfc91f1769b>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.B = torch.load(model_path / \"B_10_179999.pt\")\n",
            "100%|██████████| 2048/2048 [00:30<00:00, 68.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.07235777378082275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config = Config()\n",
        "config.n = 10\n",
        "model = ILC(config)\n",
        "model.B = torch.load(model_path / \"B_10_179999.pt\")\n",
        "#model.B.data = torch.diag(torch.ones(config.n, dtype=config.dtype, device=config.device))\n",
        "model.to(config.device)\n",
        "model.eval()\n",
        "\n",
        "eval_size = 2048\n",
        "loss = torch.tensor(0, dtype=config.dtype, device=config.device)\n",
        "z = torch.rand(eval_size, config.n, dtype=config.dtype, device=config.device)\n",
        "for i in tqdm(range(eval_size)):\n",
        "  y = calculate_y(config.n, z[i], model.B.data)\n",
        "  loss += model(y)\n",
        "print(loss.item() / eval_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zekzyCeM_bAW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
